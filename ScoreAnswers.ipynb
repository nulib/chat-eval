{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Answers\n",
    "\n",
    "This notebook provides a testing framework to evaluate responses from DC API's chat.\n",
    "\n",
    "For input, you'll need a csv file with `question`, `answer` and `ground_truth` columns. \n",
    "It will score the answers using an AWS Bedrock Claude model (either opus, haiku or sonnet) and produce a csv file additionally containing a `score`, `pass` (T/F) and `reason` columns.\n",
    "\n",
    "Adapted from this [AWS sample notebook](https://github.com/aws-samples/llm-based-advanced-summarization/blob/main/Prompt%20Evaluation.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "If you haven't already logged into AWS, close this notebook, then, in your terminal, from this project's directory, login to AWS. Once you're logged in, repopen the Jupyter notebook. For example: \n",
    "\n",
    "`export AWS_PROFILE=staging && aws sso login`\n",
    "\n",
    "`jupyter notebook`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#confirm that you have AWS credentials\n",
    "print(os.getenv('AWS_PROFILE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Environment\n",
    "\n",
    "First, import the libraries we'll need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3\n",
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, time, json, csv\n",
    "import json, os\n",
    "from urllib.parse import urljoin\n",
    "from botocore.config import Config\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#increase the standard time out limits in boto3, because Bedrock may take a while to respond to large requests.\n",
    "my_config = Config(\n",
    "    connect_timeout=60*3,\n",
    "    read_timeout=60*3,\n",
    ")\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that it's working:\n",
    "models = bedrock_service.list_foundation_models()\n",
    "for line in models[\"modelSummaries\"]:\n",
    "    #print this out if you want to see all the models you have access to.\n",
    "    print (line[\"modelId\"])\n",
    "    pass\n",
    "if \"anthropic.claude-3\" in str(models):\n",
    "    print(\"Claud-v3 found!\")\n",
    "else:\n",
    "    print (\"Error, no model found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create helper functions to send messages to Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose what model to use ('haiku', 'sonnet', or 'opus)\n",
    "model_version = 'sonnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ATTEMPTS = 3 #how many times to retry if Claude is not working.\n",
    "session_cache = {} #for this session, do not repeat the same query to claude.\n",
    "def ask_claude(messages,system=\"\", DEBUG=False, model_version=\"haiku\"):\n",
    "    '''\n",
    "    Send a prompt to Bedrock, and return the response.  Debug is used to see exactly what is being sent to and from Bedrock.\n",
    "    messages can be an array of role/message pairs, or a string.\n",
    "    '''\n",
    "    raw_prompt_text = str(messages)\n",
    "    \n",
    "    if type(messages)==str:\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    \n",
    "    promt_json = {\n",
    "        \"system\":system,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"anthropic_version\":\"\",\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    "    \n",
    "    if DEBUG: print(\"sending:\\nSystem:\\n\",system,\"\\nMessages:\\n\",\"\\n\".join(messages))\n",
    "    \n",
    "    if model_version== \"opus\":\n",
    "        modelId = 'anthropic.claude-3-opus-20240229-v1:0'\n",
    "    elif model_version== \"sonnet\":\n",
    "        modelId = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "    elif model_version== \"haiku\":\n",
    "        modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "    else:\n",
    "        print (\"ERROR:  Bad model version, must be opus, sonnet, or haiku.\")\n",
    "        modelId = 'error'\n",
    "    \n",
    "    if raw_prompt_text in session_cache:\n",
    "        return [raw_prompt_text,session_cache[raw_prompt_text]]\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            response = bedrock.invoke_model(body=json.dumps(promt_json), modelId=modelId, accept='application/json', contentType='application/json')\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            results = response_body.get(\"content\")[0].get(\"text\")\n",
    "            if DEBUG:print(\"Recieved:\",results)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error with calling Bedrock: \"+str(e))\n",
    "            attempt+=1\n",
    "            if attempt>MAX_ATTEMPTS:\n",
    "                print(\"Max attempts reached!\")\n",
    "                results = str(e)\n",
    "                break\n",
    "            else:#retry in 10 seconds\n",
    "                time.sleep(10)\n",
    "    session_cache[raw_prompt_text] = results\n",
    "    return [raw_prompt_text,results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# Threaded function for queue processing.\n",
    "def thread_request(q, result, model):\n",
    "    while not q.empty():\n",
    "        work = q.get()                      #fetch new work from the Queue\n",
    "        thread_start_time = time.time()\n",
    "        try:\n",
    "            data = ask_claude(work[1], model)\n",
    "            result[work[0]] = data          #Store data back at correct index\n",
    "        except Exception as e:\n",
    "            error_time = time.time()\n",
    "            print('Error with prompt!',str(e))\n",
    "            result[work[0]] = (str(e))\n",
    "        #signal to the queue that task has been processed\n",
    "        q.task_done()\n",
    "    return True\n",
    "\n",
    "def ask_claude_threaded(prompts, model, DEBUG=False):\n",
    "    '''\n",
    "    Call ask_claude, but multi-threaded.\n",
    "    Returns a dict of the prompts and responces.\n",
    "    '''\n",
    "    print(f\"Using model: {model}...\")\n",
    "    q = Queue(maxsize=0)\n",
    "    num_theads = min(50, len(prompts))\n",
    "    \n",
    "    #Populating Queue with tasks\n",
    "    results = [{} for x in prompts];\n",
    "    #load up the queue with the promts to fetch and the index for each job (as a tuple):\n",
    "    for i in range(len(prompts)):\n",
    "        #need the index and the url in each queue item.\n",
    "        q.put((i,prompts[i]))\n",
    "        \n",
    "    #Starting worker threads on queue processing\n",
    "    for i in range(num_theads):\n",
    "        #print('Starting thread ', i)\n",
    "        worker = Thread(target=thread_request, daemon=True, args=(q,results, model))\n",
    "        # worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to \n",
    "                                  #exit eventually even if these dont finish \n",
    "                                  #correctly.\n",
    "        worker.start()\n",
    "\n",
    "    #now we wait until the queue has been processed\n",
    "    q.join()\n",
    "\n",
    "    # if DEBUG:print('All tasks completed.')\n",
    "    print('All tasks completed.')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test that it's working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#check that Claude responses are working:\n",
    "try:\n",
    "    query = \"Please say the number four.\"\n",
    "    result = ask_claude(query)\n",
    "    print(query)\n",
    "    print(result[1])\n",
    "except Exception as e:\n",
    "    print(\"Error with calling Claude: \"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#test if our threaded Claude calls are working\n",
    "q1 = [{\"role\": \"user\", \"content\": \"Please say the number one.\"}]\n",
    "q2 = [{\"role\": \"user\", \"content\": \"Please say the number two.\"}]\n",
    "q3 = [{\"role\": \"user\", \"content\": \"Please say the number 55.\"}]\n",
    "\n",
    "print(ask_claude_threaded([q1,q2,q3], 'opus'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the scoring prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_prompt_template = \"\"\"You are a grader.  Consider the following question along with its correct answer or ground truth and a submitted answer to grade.\n",
    "Here is the question:\n",
    "<question>{{QUESTION}}</question>\n",
    "Here is the correct answer:\n",
    "<ground_truth>{{GROUND_TRUTH}}</ground_truth>\n",
    "Here is the submitted answer:\n",
    "<answer>{{ANSWER}}</answer>\n",
    "Please provide a score from 0 to 100 on how well this answer matches the correct answer for this question.\n",
    "The score should be high if the answers say essentially the same thing.\n",
    "The score should be lower if some facts are missing or incorrect, or if extra unnecessary facts have been included.\n",
    "The score should be 0 for entirely wrong answers.  Put the score in <SCORE> tags. and your reasoning in <REASON> tags.\n",
    "Do not consider your own answer to the question, but instead score based on the ground_truth above.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the score answers function\n",
    "def score_answers(prompt_template, question_answers, model):\n",
    "    '''\n",
    "    ask our LLM to score each of the generated answers.\n",
    "    '''\n",
    "    \n",
    "    prompts = []\n",
    "    for question in question_answers:  \n",
    "        print(f\"Scoring: {question['question']}...\")\n",
    "        prompts.append(scoring_prompt_template.replace(\"{{QUESTION}}\", question[\"question\"]).replace(\"{{GROUND_TRUTH}}\",question[\"ground_truth\"]).replace(\"{{ANSWER}}\",question[\"answer\"]))\n",
    "    return ask_claude_threaded(prompts, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test that it's working with fake data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question_set = [\n",
    "    {'question': \"What color is green?\", 'ground_truth': \"It's green\", \"answer\": \"It's between yellow and blue and can be grassy or olive\"},\n",
    "    {'question': \"Where am I?\", 'ground_truth': \"Here.\", \"answer\": \"You are neither here nor there. You are everywhere.\"},\n",
    "    {'question': \"Do dogs like cats?\", 'ground_truth': \"No.\", \"answer\": \"More than cats like dogs.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_result = score_answers(scoring_prompt_template, test_question_set, model_version)\n",
    "print(fake_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure input source and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the path to your input file here\n",
    "input_filename = 'output_files/20240917115836/40_realistic_with_ground_truth.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input data\n",
    "csvfile = csv.DictReader(open(input_filename, \"r\", newline='', encoding='utf_8_sig') )\n",
    "question_set = list(csvfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify it's loaded\n",
    "print(question_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the function to score the answers\n",
    "def evaluate_prompt(prompt_template, question_answers, threshhold):\n",
    "    \"\"\"\n",
    "    Call score answers and format the results once all threads have returned.\n",
    "    \"\"\"\n",
    "    scored_answers = score_answers(prompt_template, question_answers, model_version)\n",
    "    print (\"Done.\")\n",
    "    \n",
    "    scores = []\n",
    "    scores.append(['question','ground_truth','answer','score','reason','passed'])\n",
    "    for prompt,response in scored_answers:\n",
    "        soup = BS(prompt)\n",
    "        question = soup.find('question').text\n",
    "        ground_truth = soup.find('ground_truth').text\n",
    "        answer = soup.find('answer').text\n",
    "        soup = BS(response)\n",
    "        score = soup.find('score').text\n",
    "        reason = soup.find('reason').text\n",
    "        passed = True\n",
    "        if int(score)<threshhold:\n",
    "            passed = False\n",
    "        scores.append([question,ground_truth,answer,score,reason,passed])\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what score would be passing\n",
    "threshold_to_pass = 90\n",
    "\n",
    "# Run the evaluation\n",
    "scores = evaluate_prompt(scoring_prompt_template, question_set,threshhold=threshold_to_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can take a second\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "os.makedirs(os.path.join('output_files/scored', timestamp), exist_ok=True)\n",
    "output_base_path = f\"output_files/scored/{timestamp}\"\n",
    "filename = os.path.join(output_base_path, f\"{os.path.splitext(os.path.basename(input_filename))[0]}.csv\")\n",
    "\n",
    "with open(filename, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(scores)\n",
    "\n",
    "print(f\"Output files saved to: {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
